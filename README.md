# Neural Networks and Deep Learning
- **Neural networks** : A beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data
- **Deep learning** : A powerful set of techniques for learning in neural networks

## Using neural nets to recognize handwritten digits
- Perceptrons
- Sigmoid neurons
- The architecture of neural networks
- A simple networks to classify handwritten digits
- Learning with gradient descent
- Implementing our network to classifiy digits
- Toward deep learning

## How the backpropagation algorithm works
- Warm up: a fast matrix - based approach to computing the output from a neural network
- The two assumptions we need about the cost function
- The Hadamard products
- The four fundamental equations behind backpropagation
- Proof of the four fundamental equations
- The backpropagation algorithm
- The code for backpropagation
- In what sense is backpropagation a fast algorithm?
- Backpropagation: the big picture

## Improving the way neural networks learn 
- The cross-entropy cost function
- Overfitting and regularization
- Weight initialization
- Handwriting recognition revisited: the code
- How to choose a neural network's hyper-parameters?
- Other techniques

## A visual proof that neural nets can compute any function
- Two caveats
- Universality with one input and one output
- Many input variables
- Extension beyond sigmoid neurons
- Fixing up the step functions
- Conclusion

## Why are deep neural networks hard to train?
- The vanishing gradient problem
- What's causing the vanishing gradients in neural nets
- Unstable gradients in more complex networks
- Other obstacles to deep learning

## Deep Learning
- Introduction convolutional networks
- Convolutional neural networks in practice
- The code for our convolutional networks
- Recent progress in image recognition
- Other approaches to deep neural nets
- On the future of neural networks
